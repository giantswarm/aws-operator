apiVersion: "provider.giantswarm.io/v1alpha1"
kind: AzureConfig
metadata:
  name: "{{ .Values.clusterName }}"
  namespace: "default"
  labels:
    giantswarm.io/cluster: "{{ .Values.clusterName }}"
spec:
  azure:
    credentialSecret:
      name: "credential-default"
      namespace: "giantswarm"
    dnsZones:
      api:
        name: "{{ .Values.commonDomain }}"
        resourceGroup: "{{ .Values.commonDomainResourceGroup }}"
      etcd:
        name: "{{ .Values.commonDomain }}"
        resourceGroup: "{{ .Values.commonDomainResourceGroup }}"
      ingress:
        name: "{{ .Values.commonDomain }}"
        resourceGroup: "{{ .Values.commonDomainResourceGroup }}"
    location: "{{ .Values.azure.location }}"
    masters:
    - vmSize: "{{ .Values.azure.vmSizeMaster }}"
    workers:
    - vmSize: "{{ .Values.azure.vmSizeWorker }}"
    - vmSize: "{{ .Values.azure.vmSizeWorker }}"
    virtualNetwork:
      calicoSubnetCIDR: "{{ .Values.azure.calicoSubnetCIDR }}"
      cidr: "{{ .Values.azure.cidr }}"
      masterSubnetCIDR: "{{ .Values.azure.masterSubnetCIDR }}"
      workerSubnetCIDR: "{{ .Values.azure.workerSubnetCIDR }}"
  cluster:
    calico:
      cidr: 16
      domain: "calico.{{ .Values.clusterName }}.k8s.{{ .Values.commonDomain }}"
      mtu: 1500
    customer:
      id: "example-customer"
    docker:
      daemon:
        cidr: "172.17.0.1/16"
        extraArgs: "--log-opt max-size=25m --log-opt max-file=2 --log-opt labels=io.kubernetes.container.hash,io.kubernetes.container.name,io.kubernetes.pod.name,io.kubernetes.pod.namespace,io.kubernetes.pod.uid"
    etcd:
      domain: "etcd.{{ .Values.clusterName }}.k8s.{{ .Values.commonDomain }}"
      prefix: "giantswarm.io"
      port: 2379
    id: "{{ .Values.clusterName }}"
    kubernetes:
      api:
        altNames: "kubernetes,kubernetes.default,kubernetes.default.svc,kubernetes.default.svc.cluster.local"
        clusterIPRange: "172.31.0.0/16"
        domain: "api.{{ .Values.clusterName }}.k8s.{{ .Values.commonDomain }}"
        insecurePort: 8080
        ip: "172.31.0.1"
        securePort: 443
      cloudProvider: "azure"
      dns:
        ip: "172.31.0.10"
      domain: "cluster.local"
      ingressController:
        insecurePort: 30010
        securePort: 30011
        domain: "ingress.{{ .Values.clusterName }}.k8s.{{ .Values.commonDomain }}"
        wildcardDomain: "*.{{ .Values.clusterName }}.k8s.{{ .Values.commonDomain }}"
      kubelet:
        altNames: "kubernetes,kubernetes.default,kubernetes.default.svc,kubernetes.default.svc.cluster.local"
        domain: "worker.{{ .Values.clusterName }}.k8s.{{ .Values.commonDomain }}"
        labels: "giantswarm.io/provider=azure,azure-operator.giantswarm.io/version={{ .Values.versionBundleVersion }}"
        port: 10250
      networkSetup:
        docker:
          image: "quay.io/giantswarm/k8s-setup-network-environment:1f4ffc52095ac368847ce3428ea99b257003d9b9"
      ssh:
        userList:
        - name: "{{ .Values.sshUser }}"
          publicKey: "{{ .Values.sshPublicKey }}"
  versionBundle:
    version: "{{ .Values.versionBundleVersion }}"
